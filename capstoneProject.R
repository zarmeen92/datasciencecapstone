library(tm)
library(stringi)
library(RWeka)
library(ggplot2) 
library(wordcloud)
library(dplyr)
library(SnowballC)   
library(stringr)

setwd('D:\\DataScience\\Capstone')
## Reading data
con <- file("D:\\DataScience\\Capstone\\Coursera-SwiftKey\\final\\en_US\\en_US.twitter.txt", "r")
con1 <- file("D:\\DataScience\\Capstone\\Coursera-SwiftKey\\final\\en_US\\en_US.news.txt", "r")
con2 <- file("D:\\DataScience\\Capstone\\Coursera-SwiftKey\\final\\en_US\\en_US.blogs.txt", "r")

tweets <- readLines(con,  encoding = "UTF-8", skipNul=TRUE) 
news <- readLines(con1,  encoding = "UTF-8", skipNul=TRUE) 
blogs <- readLines(con2,  encoding = "UTF-8", skipNul=TRUE)
close(con)
close(con1)
close(con2)

corpus_stats <- data.frame(File = c("blogs","news","twitter"),t(rbind(sapply(list(blogs,news,tweets),stri_stats_general),
          TotalWords = sapply(list(blogs,news,tweets),stri_stats_latex)[4,]))
)
print(corpus_stats)
set.seed(12)
sampleCorpus <- list()
tweetsSample <- tweets[sample(1:length(tweets),20000)]
newsSample <- news[sample(1:length(news),20000)]
blogsSample <- blogs[sample(1:length(blogs),20000)]
rm(tweets,news,blogs)
sampleCorpus <- c(tweetsSample,newsSample,blogsSample)
# Remove non print data etc
sampleCorpus  <- iconv(sampleCorpus, "latin1", "ASCII", sub=" ");
sampleCorpus  <- gsub("[^[:alpha:][:space:][:punct:]]", "",sampleCorpus);


writeLines(sampleCorpus, "./corpus/sampleCorpus.txt")
length(sampleCorpus)

badwords <- readLines('badwords.txt')
directory <- file.path(".", "corpus")
docs <- Corpus(DirSource(directory,encoding = "UTF-8"),readerControl=list(language="en"))

## Preprocessing      
docs <- tm_map(docs, removePunctuation)   # *Removing punctuation:*    
docs <- tm_map(docs, removeNumbers)      # *Removing numbers:*    
docs <- tm_map(docs, tolower)   # *Converting to lowercase:*    
#docs <- tm_map(docs, removeWords, stopwords("english"))   # *Removing "stopwords" 
docs <- tm_map(docs, removeWords, badwords)   # *Removing "badwords" 
#docs <- tm_map(docs, stemDocument)   # *Removing common word endings* (e.g., "ing", "es")   
docs <- tm_map(docs, stripWhitespace)   # *Stripping whitespace   
docs <- tm_map(docs, PlainTextDocument) 

## Saving the final corpus
saveRDS(docs, file = "./cleanCorpus.RDS")


## Exploratory analysis 

## Building the n-grams

docs <- readRDS("./cleanCorpus.RDS")

unigramTokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min = 1, max = 1))
}
unigrams <- DocumentTermMatrix(docs, control = list(tokenize = unigramTokenizer))
BigramTokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
bigrams <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))

saveRDS(bigrams, file = "./bigrams.RDS")
#bigrams
print("processing bigrams")
#first compute the total number of frequencies of each word across all documents
freq_B <- colSums(as.matrix(bigrams))  #we use rowSums as the terms are along the rows
#create a dataframe
wf_B <- data.frame(word=names(freq_B),count=freq_B,stringsAsFactors=FALSE)

#find the start and end of bigrams 
biL <- str_split(wf_B$word," ")
wf_B$start <- sapply(biL,FUN=function(x) x[1]) #starting unigram
wf_B$end <- sapply(biL,FUN=function(x) x[2]) #ending unigram

saveRDS(wf_B, file = "./wf_B.RDS")


#trigrams
print("processing trigrams")
TrigramTokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min = 3, max = 3))
}

trigrams <- DocumentTermMatrix(docs, control = list(tokenize = TrigramTokenizer))
saveRDS(trigrams, file = "./trigrams.RDS")
#first compute the total number of frequencies of each word across all documents
freq_T <- colSums(as.matrix(trigrams))  #we use rowSums as the terms are along the rows
#create a dataframe
wf_T <- data.frame(word=names(freq_T),count=freq_T,stringsAsFactors=FALSE)
#expland the wf_T DF -- store the starting bigram and the ending word
triL <- str_split(wf_T$word," ")
wf_T$start <- sapply(triL,FUN=function(x) paste(x[1],x[2]))
wf_T$end <- sapply(triL,FUN=function(x) x[3])

saveRDS(wf_T, file = "./wf_T.RDS")




#quadgrams
print("processing 4grams")
QuadgramTokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min = 4, max = 4))
}

quadgrams <- DocumentTermMatrix(docs, control = list(tokenize = QuadgramTokenizer))
saveRDS(quadgrams, file = "./quadgrams.RDS")
#first compute the total number of frequencies of each word across all documents
freq_Q <- colSums(as.matrix(quadgrams))  #we use rowSums as the terms are along the rows
#create a dataframe
wf_Q <- data.frame(word=names(freq_Q),count=freq_Q,stringsAsFactors=FALSE)
#expland the wf_T DF -- store the starting bigram and the ending word
quaL <- str_split(wf_Q$word," ")
wf_Q$start <- sapply(quaL,FUN=function(x) paste(x[1],x[2],x[3]))
wf_Q$end <- sapply(quaL,FUN=function(x) x[4])

saveRDS(wf_T, file = "./wf_Q.RDS")


SimpleGT <- function(table_N){
  #Simple Good Turing Algorithm - Gale And Simpson
  #Good Turing Smoothing
  
  # table_U is a table of frequency of frequencies
  # The frequencies are stored as names of the list in the table structure
  # the values are the frequency of frequencies.
  # In Good Turing Smoothing, we are concerned with the frequency of frequencies
  # So, to extract the number of times that words of frequency n occur in the training set, we need to do:
  # table(freq_B)[[as.character(pairCount)]]
  # In a tables with a number of holes or non-contiguous sequence of frequency of words,
  # we can compute N(c+1) as the mean of all frequencies that occur more than Nc times
  # to do this, create a vector that is in the numerical form of the names of the table
  
  # create a data table
  # r is the frequencies of various trigrams
  #n is the frequency of frquencies
  SGT_DT <- data.frame(r=as.numeric(names(table_N)),n=as.vector(table_N),Z=vector("numeric",length(table_N)), 
                       logr=vector("numeric",length(table_N)),
                       logZ=vector("numeric",length(table_N)),
                       r_star=vector("numeric",length(table_N)),
                       p=vector("numeric",length(table_N)))
  #p=vector("numeric",length(table_N)),key="r")
  
  str(SGT_DT)
  
  num_r <- nrow(SGT_DT)
  for (j in 1:num_r) {
    if(j==1) {r_i<-0} else {r_i <- SGT_DT$r[j-1]}
    if(j==num_r){r_k<-SGT_DT$r[j]} else {r_k <- SGT_DT$r[j+1]}
    SGT_DT$Z[j] <- 2*SGT_DT$n[j] / (r_k-r_i)
    #print(paste(r_i,j,r_k))
  }
  SGT_DT$logr <- log(SGT_DT$r)
  SGT_DT$logZ <- log(SGT_DT$Z)
  linearFit <- lm(SGT_DT$logZ ~ SGT_DT$logr)
  print(linearFit$coefficients)
  c0 <- linearFit$coefficients[1]
  c1 <- linearFit$coefficients[2]
  
  plot(SGT_DT$logr, SGT_DT$logZ)
  abline(linearFit,col="red")
  
  use_y = FALSE
  for (j in 1:(num_r-1)) {
    r_plus_1 <- SGT_DT$r[j] + 1
    
    s_r_plus_1 <- exp(c0 + (c1 * SGT_DT$logr[j+1]))
    s_r <- exp(c0 + (c1 * SGT_DT$logr[j]))
    y<-r_plus_1 * s_r_plus_1/s_r
    
    if(use_y) {
      SGT_DT$r_star[j] <- y
    } else { 
      n_r_plus_1 <- SGT_DT$n[SGT_DT$r == r_plus_1]
      n_r <- SGT_DT$n[j]
      x<-(r_plus_1) * n_r_plus_1/n_r
      
      if (abs(x-y) > 1.96 * sqrt(((r_plus_1)^2) * (n_r_plus_1/((n_r)^2))*(1+(n_r_plus_1/n_r)))) {
        SGT_DT$r_star[j] <- x
      }else {
        SGT_DT$r_star[j] <- y
        use_y = TRUE
      }
    }
    if(j==(num_r-1)) {
      SGT_DT$r_star[j+1] <- y
    }
    
  }
  N <- sum(SGT_DT$n * SGT_DT$r)
  Nhat <- sum(SGT_DT$n * SGT_DT$r_star)
  Po <- SGT_DT$n[1] / N
  SGT_DT$p <- (1-Po) * SGT_DT$r_star/Nhat
  
  return(SGT_DT)
  
}


wf_B <- readRDS(file = "./wf_B.RDS")
wf_T <- readRDS(file = "./wf_T.RDS")
wf_Q <- readRDS(file = "./wf_Q.RDS")
tri_SGT_DT <- readRDS(file = "./tri_SGT_DT.RDS")
quad_SGT_DT <- readRDS(file = "./quad_SGT_DT.RDS")
bi_SGT_DT <- readRDS(file = "./bi_SGT_DT.RDS")


# #SGT tables for bigrams and trigrams
tri_SGT_DT <- SimpleGT(table(freq_T))
saveRDS(tri_SGT_DT, file = "./tri_SGT_DT.RDS")

bi_SGT_DT <- SimpleGT(table(freq_B))
saveRDS(bi_SGT_DT, file = "./bi_SGT_DT.RDS")

# #SGT tables for bigrams and trigrams
quad_SGT_DT <- SimpleGT(table(freq_Q))
saveRDS(quad_SGT_DT, file = "./quad_SGT_DT.RDS")

dataCleaner<-function(text){
  
  cleanText <- tolower(text)
  cleanText <- removePunctuation(cleanText)
  cleanText <- removeNumbers(cleanText)
  cleanText <- str_replace_all(cleanText, "[^[:alnum:]]", " ")
  cleanText <- stripWhitespace(cleanText)
  #cleanText <- removeWords(cleanText,stopwords("english"))
  
  return(cleanText)
}


predict <- function(sentence) {
  # given a sentence/phrase, extract the last two words
  sentence <- dataCleaner(sentence)
  print(sentence)
  sl <- unlist(str_split(sentence," "))
  len <- length(sl)
  bigram <- paste(sl[len-1],sl[len])
  bigram
  
  # get the subset of the trigram data table witha matching bigram start
  swf_T <- wf_T[wf_T$start == bigram,]
  #check if bigram was found in the trigram table
  if(nrow(swf_T) > 0) {
    # use the counts in the Simple GT table to extract the probability
    swf_T$p <- sapply(swf_T$count,FUN=function(x) tri_SGT_DT$p[tri_SGT_DT$r==x])
    # order by probability
    #swf_T <- swf_T[with(swf_T,order(-p))]
    # find the largest probability
    maxP <-max(swf_T$p)
    #get the end words with the highest probability
    predictList <- swf_T$end[swf_T$p == maxP]
    predictions <- vector()
    for(i in 1:length(predictList)) {
      predictions[i] <- paste(sentence,predictList[i])
    }
    return(predictions)
    #pl_T <- data.frame(words=swf_T$end,probs=swf_T$p)
    #return(pl_T)
  } else {
    print(paste("No match for bigram",bigram,"in",sentence,"--looking for unigram match"))
    unigram <- sl[len]
    unigram
    swf_B <- wf_B[wf_B$start == unigram,]
    if(nrow(swf_B) > 0) {
      # use the counts in the Simple GT table to extract the probability
      swf_B$p <- sapply(swf_B$count,FUN=function(x) bi_SGT_DT$p[bi_SGT_DT$r==x])
      # order by probability
      swf_B <- swf_B[with(swf_B,order(-p)),]
      # find the largest probability
      maxP <-max(swf_B$p)
      #get the end words with the highest probability
      predictList <- swf_B$end[swf_B$p == maxP]
      predictions <- vector()
      
      for(i in 1:length(predictList)) {
        predictions[i] <- paste(sentence,predictList[i])
      }
      pl_B <- data.frame(words=swf_B$end,probs=swf_B$p)
      return(pl_B[1:10,])
    } else {
      print(paste("No match for unigram",unigram,"in",sentence))
    }
  }
  
}



#predict using 4gram
predict4gram <- function(sentence) {
  # given a sentence/phrase, extract the last two words
  sentence <- dataCleaner(sentence)
  print(sentence)
  sl <- unlist(str_split(sentence," "))
  len <- length(sl)
  trigram <- paste(sl[len-2],sl[len-1],sl[len])
  trigram
  
  # get the subset of the quadgram data table witha matching trigram start
  swf_Q <- wf_Q[wf_Q$start == trigram,]
  #check if trigram was found in the quadgram table
  if(nrow(swf_Q) > 0) {
    # use the counts in the Simple GT table to extract the probability
    swf_Q$p <- sapply(swf_Q$count,FUN=function(x) quad_SGT_DT$p[quad_SGT_DT$r==x])
    # order by probability
    #swf_T <- swf_T[with(swf_T,order(-p))]
    # find the largest probability
    maxP <-max(swf_Q$p)
    #get the end words with the highest probability
    predictList <- swf_Q$end[swf_Q$p == maxP]
    predictions <- vector()
    for(i in 1:length(predictList)) {
      predictions[i] <- paste(sentence,predictList[i])
    }
   # return(predictions)
    pl_T <- data.frame(words=swf_Q$end,probs=swf_Q$p)
    return(pl_T[1:20,])
  } else {
    print(paste("No match for trigram",trigram,"in",sentence,"--looking for bigram match"))
    bigram <- paste(sl[len-1],sl[len])
    bigram
    swf_B <- wf_T[wf_T$start == bigram,]
    if(nrow(swf_B) > 0) {
      # use the counts in the Simple GT table to extract the probability
      swf_B$p <- sapply(swf_B$count,FUN=function(x) tri_SGT_DT$p[tri_SGT_DT$r==x])
      # order by probability
      swf_B <- swf_B[with(swf_B,order(-p)),]
      # find the largest probability
      maxP <-max(swf_B$p)
      #get the end words with the highest probability
      predictList <- swf_B$end[swf_B$p == maxP]
      predictions <- vector()
      
      for(i in 1:length(predictList)) {
        predictions[i] <- paste(sentence,predictList[i])
      }
      pl_B <- data.frame(words=swf_B$end,probs=swf_B$p)
      return(pl_B[1:10,])
    } else {
      print(paste("No match for bigram",bigram,"in",sentence,"--looking for unigram match"))
      unigram <- sl[len]
      unigram
      swf_B <- wf_B[wf_B$start == unigram,]
      if(nrow(swf_B) > 0) {
        # use the counts in the Simple GT table to extract the probability
        swf_B$p <- sapply(swf_B$count,FUN=function(x) bi_SGT_DT$p[bi_SGT_DT$r==x])
        # order by probability
        swf_B <- swf_B[with(swf_B,order(-p)),]
        # find the largest probability
        maxP <-max(swf_B$p)
        #get the end words with the highest probability
        predictList <- swf_B$end[swf_B$p == maxP]
        predictions <- vector()
        
        for(i in 1:length(predictList)) {
          predictions[i] <- paste(sentence,predictList[i])
        }
        pl_B <- data.frame(words=swf_B$end,probs=swf_B$p)
        return(pl_B[1:10,])
      }
      else {
        print(paste("No match for unigram",unigram,"in",sentence))
      }
    }
  }
  
}


# Example
predict4gram("how was your")

